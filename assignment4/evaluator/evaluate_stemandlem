#!/usr/bin/env python
from __future__ import division 
import argparse # optparse is deprecated
from itertools import islice # slicing for iterators
from nltk.corpus import wordnet
import nltk
import sys  

reload(sys)  
sys.setdefaultencoding('utf8')
def findSyn(sentence):
#    print "findsyn"
    synlist = []
 #   print "sentence", sentence
    for word in sentence:
#     try:
        for synset in wordnet.synsets(word):
            for syn in synset.lemma_names():
                if syn.decode('utf8') not in synlist:
                    synlist.append(syn.decode('utf8'))
 #    except UnicodeDecodeError
        if word not in synlist:
            synlist.append(word)
#    print "synlist",synlist
    return synlist
def word_matches(h, ref):
    return sum(1 for w in h if w in ref)

lemmatizer = nltk.WordNetLemmatizer()
stemmer = nltk.stem.porter.PorterStemmer()

def normaliseLem(word):
    """Normalises words to lowercase and stems and lemmatizes it."""
    word = word.lower()
    word = lemmatizer.lemmatize(word)
    return word 
def normaliseStem(word):
    """Normalises words to lowercase and stems and lemmatizes it."""
    word = word.lower()
    word = stemmer.stem_word(word)
    return word 
def normaliseLemnStem(word):
    """Normalises words to lowercase and stems and lemmatizes it."""

    word = word.lower()
    word = stemmer.stem_word(word)
    word = lemmatizer.lemmatize(word)
    return word 


def main():
    parser = argparse.ArgumentParser(description='Evaluate translation hypotheses.')
    parser.add_argument('-i', '--input', default='data/hyp1-hyp2-ref',
            help='input file (default data/hyp1-hyp2-ref)')
    parser.add_argument('-n', '--num_sentences', default=None, type=int,
            help='Number of hypothesis pairs to evaluate')
    # note that if x == [1, 2, 3], then x[:None] == x[:] == x (copy); no need for sys.maxint
    opts = parser.parse_args()
 
    # we create a generator and avoid loading all sentences into a list
    def sentences():
        with open(opts.input) as f:
            for pair in f:
                yield [sentence.strip().split() for sentence in pair.split(' ||| ')]
 
    a = 0.1
    for h1, h2, ref in islice(sentences(), opts.num_sentences):
        rset = set(ref)
        h1_match = meteor(h1,rset,a) 
        h2_match = meteor(h2,rset,a)
        result = (1 if h1_match[0] > h2_match[0] else # \begin{cases}
                (0 if h1_match[0] == h2_match[0]
                    else -1)) # \end{cases}
        with open('result_LemnStem', 'a') as f:
            f.write("%s\n" % str(result))
        result = (1 if h1_match[1] > h2_match[1] else # \begin{cases}
                (0 if h1_match[1] == h2_match[1]
                    else -1)) # \end{cases}
        with open('result_Lem', 'a') as f:
            f.write("%s\n" % str(result))
        result = (1 if h1_match[2] > h2_match[2] else # \begin{cases}
                (0 if h1_match[2] == h2_match[2]
                    else -1)) # \end{cases}
        with open('result_Stem', 'a') as f:
            f.write("%s\n" % str(result))
            
def meteor(h,e,a):
    l = []
    precision,recall = findPrecisionRecall(h,e)
    for i in range (0,3):
        try:
            l.append((precision[i]*recall[i]) / (((1-a)*recall[i]) + a*precision[i]))
        except ZeroDivisionError:
            l.append(-1)
    return (l)

def findPrecisionRecall(h,e):
    countcommonls = 0
    countcommonl = 0
    countcommons = 0
    sizeh = len(h)
    sizee = len(e)
    synset = findSyn(e)
    synstemls = []
    synsteml = []
    synstems = []
    recall = []
    precision = []
    for word in synset:
        synstemls.append(normaliseLemnStem(word.decode('utf8')))
        synsteml.append(normaliseLem(word.decode('utf8')))
        synstems.append(normaliseStem(word.decode('utf8')))
    for word in h:
        if normaliseLemnStem(word.decode('utf8')) in synstemls:
            countcommonls += 1
        if normaliseLem(word.decode('utf8')) in synsteml:
            countcommonl += 1
        if normaliseStem(word.decode('utf8')) in synstems:
            countcommons += 1
    recall.append(countcommonls/sizee)
    precision.append(countcommonls/sizeh)
    recall.append(countcommonl/sizee)
    precision.append(countcommonl/sizeh)
    recall.append(countcommons/sizee)
    precision.append(countcommons/sizeh)
    return (precision,recall)
# convention to allow import of this file as a module
if __name__ == '__main__':
    main()
