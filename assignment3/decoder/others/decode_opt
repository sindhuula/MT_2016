#!/usr/bin/env python
import optparse
import sys
import models
from collections import namedtuple

optparser = optparse.OptionParser()
optparser.add_option("-i", "--input", dest="input", default="data/input", help="File containing sentences to translate (default=data/input)")
optparser.add_option("-t", "--translation-model", dest="tm", default="data/tm", help="File containing translation model (default=data/tm)")
optparser.add_option("-l", "--language-model", dest="lm", default="data/lm", help="File containing ARPA-format language model (default=data/lm)")
optparser.add_option("-n", "--num_sentences", dest="num_sents", default=sys.maxint, type="int", help="Number of sentences to decode (default=no limit)")
optparser.add_option("-k", "--translations-per-phrase", dest="k", default=20, type="int", help="Limit on number of translations to consider per phrase (default=1)")
optparser.add_option("-s", "--stack-size", dest="s", default=100, type="int", help="Maximum stack size (default=1)")
optparser.add_option("-v", "--verbose", dest="verbose", action="store_true", default=False,  help="Verbose mode (default=off)")

opts = optparser.parse_args()[0]

tm = models.TM(opts.tm, opts.k)
lm = models.LM(opts.lm)
french = [tuple(line.strip().split()) for line in open(opts.input).readlines()[:opts.num_sents]]

#parameters
stepsize = 2.0
eta = -1.5
alpha_decrease_rate = 0.9
epsilon = 0.02
iteration_limit = 10
Constraints_limit = 2

# tm should translate unknown words as-is with probability 1
for word in set(sum(french,())):
  if (word,) not in tm:
    tm[(word,)] = [models.phrase(word, 0.0)]

sys.stderr.write("Decoding %s...\n" % (opts.input,))

def beam(f, u, Constraints):
  # The following code implements a monotone decoding
  # algorithm (one that doesn't permute the target phrases).
  # Hence all hypotheses in stacks[i] represent translations of 
  # the first i words of the input sentence. You should generalize
  # this so that they can represent translations of *any* i words.
  hypothesis = namedtuple("hypothesis", "logprob, lm_state, predecessor, phrase")
  initial_hypothesis = hypothesis(0.0, lm.begin(), None, None)
  stacks = [{} for _ in f] + [{}]
  stacks[0][lm.begin()] = initial_hypothesis
  for i, stack in enumerate(stacks[:-1]):
    for h in sorted(stack.itervalues(),key=lambda h: -h.logprob)[:opts.s]: # prune
      for j in xrange(i+1,len(f)+1):
        if f[i:j] in tm:
          for phrase in tm[f[i:j]]:
            logprob = h.logprob + phrase.logprob
            lm_state = h.lm_state
            for word in phrase.english.split():
              (lm_state, word_logprob) = lm.score(lm_state, word)
              logprob += word_logprob
            logprob += lm.end(lm_state) if j == len(f) else 0.0
            new_hypothesis = hypothesis(logprob, lm_state, h, phrase)
            if lm_state not in stacks[j] or stacks[j][lm_state].logprob < logprob: # second case is recombination
              stacks[j][lm_state] = new_hypothesis

  #start at i+1 and go to same j!!! or j+1 to len(f)+1
        for l in xrange(i+1, j):
          #split phrase [i:l] [l:j]
          if f[i:l] in tm and f[l:j] in tm:
            #check each phrase the same way as above
            for phrase1 in tm[f[i:l]]:
              # phrase 2 same way as above
              for phrase2 in tm[f[l:j]]:
                logprob1 = h.logprob + phrase2.logprob
                lm_state1 = h.lm_state
                for word in phrase2.english.split():
                  (lm_state1, word_logprob1) = lm.score(lm_state1, word)
                  logprob1 += word_logprob1
                new_hypothesis1 = hypothesis(logprob1, lm_state1, h, phrase2)
              #phrase 1 same way as above but with logprob1
              logprob2 = logprob1 + phrase1.logprob
              lm_state2 = lm_state1
              for word in phrase1.english.split():
                (lm_state2, word_logprob2) = lm.score(lm_state2, word)
                logprob2 += word_logprob2
              logprob2 += lm.end(lm_state2) if j == len(f) else 0.0
              new_hypothesis2 = hypothesis(logprob2, lm_state2, new_hypothesis1, phrase1)
              if lm_state2 not in stacks[j] or stacks[j][lm_state2].logprob < logprob2 and logprob1 < logprob2: # second case is recombination
                stacks[j][lm_state2] = new_hypothesis2

  winner = max(stacks[-1].itervalues(), key=lambda h: h.logprob)
  # Converts a hypothesis into a list of phrases.
  # Input:  hyp - hypothesis
  # Output: ps  - [(phrase, french)] e.g. [(phrase(english='honourable', logprob=0.0), ('honorables',))]
  # print hyp_to_phrases(winner)
  def hyp_to_phrases(self, hyp):
		phrases = []
		def get_phrases(hyp, ps):
			if hyp == None:
				return
			else:
				ps.insert(0, (hyp.phrase, hyp.fphrase))
				get_phrases(hyp.predecessor, ps)
		get_phrases(hyp, phrases)
		return phrases

  def extract_english(h): 
    return "" if h.predecessor is None else "%s%s " % (extract_english(h.predecessor), h.phrase.english)

def optimize(f, u, Constraints):
  violation_count = [0 for i in range(len(f))]
  previous = -1e10
  lstar = (-1, -1e10)
  converged = False
  

  def isoptimal(y):
    for yi in y:
      if yi != 1:
        return False
    return True

  def update_count(y):
    for (i,yi) in enumerate(y):
      if yi != 1:
        violation_count[i] += 1
  now = 0
  step = stepsize
  while not converged:
    result = beam(f, u, Constraints)
    y = result[2]
    if isoptimal(result[2]):
      return result
    if lstar[1] < result[0]:
      if (result[0] - lstar[1])/(now - lstar[0]) < epsilon:
        converged = True
      else:
        lstar = (now, result[0])
    for (i, yi) in enumerate(y):
      u[i] -= step * (yi - 1)
    if result[0] > previous:
      step = alpha_decrease_rate * step
    previous = result[0]
    now += 1
    if (now - lstar[0] > iteration_limit):
      break

  K = iteration_limit
  G = Constraints_limit
  # Find the most violated element in y vector
  for iter_count in range(K):
    result = beam(f, u, Constraints)
    y = result[2]
    #update count
    if isoptimal(y):
      return result
    update_count(y)

    #Update u
    for (i, yi) in enumerate(y):
      u[i] -= step * (yi - 1)
    #Update stepsize
    if result[0] >previous:
      step = alpha_decrease_rate * step
    previous = result[0]

  new_Constraints = Constraints[:]

  #Add G constraints
  for g in range(G):
    maxcount = -1
    maxindex = -1
    for (i, vi) in enumerate(violation_count):
      if (i-1) not in new_Constraints and i not in new_Constraints and (i+1) not in new_Constraints:
        if maxcount < vi:
          maxcount = vi
          maxindex = i
    new_Constraints.append(maxindex)
    new_Constraints.sort()
  
  # print new_Constraints
  # print 'Constraint = ', new_Constraints    
  
  return optimize(f, u, new_Constraints)
  
def main():

  with open('lr_output_luckey150','w') as f:
    for fsen in french:
      u = [0 for i in range(len(fsen))]
      result = optimize(fsen, u, [])
      print result[1]
      print >>f, result[1]
  f.close()

if __name__ == '__main__':
  main()
