'''
Tries different optimizations(greedy and stack). Works better than most algos.
'''
#!/usr/bin/env python
import optparse
import sys
import models
from collections import namedtuple
import copy


#Decode a source sentence string into a target sentence string.
def decode(source,lm,tm,opts):
	root = stack_decode(source,lm,tm,opts)
	decoded = greedy_decode(source,root,lm,tm,opts)
	return print_phrases(decoded)

# Converts a hypothesis into a list of phrases.
def hypothesis_phrase(hyp):
	phrases = []
	def get_phrases(hyp,ps):
		if hyp == None:
			return
		else:
			ps.insert(0,(hyp.phrase,hyp.fphrase))
			get_phrases(hyp.predecessor,ps)
	get_phrases(hyp,phrases)
	return phrases

# Decodes a sentence with a greedy,hill-climbing decoder. 
def greedy_decode(source,root,lm,tm,opts):
	iters = 100
	current = root
	for i in xrange(iters):
	  s_current = score(current,source,lm,tm,opts)
	  s = s_current
	  for h in find_neighbors(current,lm,tm,opts):
	    c = score(h,source,lm,tm,opts)
	    if c > s:
	      s = c
	      best = h
	  if s == s_current:
	    return current
	  else:
	    current = best
	return current

# Scores a translation using the language and translation models.
def score(ps,source,lm,tm,opts):
  lm_prob = 0.0
  lm_state = lm.begin()
  num_f_translated = 0
  for n,(ep,fp) in enumerate(ps):
    if ep != None and fp != None:
      num_f_translated += len(fp)
      for word in ep.english.split():
        (lm_state,word_logprob) = lm.score(lm_state,word)
        lm_prob += word_logprob
      lm_prob += lm.end(lm_state) if num_f_translated == len(source) else 0.0

  tm_prob = 0.0
  for (ep,fp) in ps:
    if ep != None:
      tm_prob += ep.logprob
  return (lm_prob + tm_prob)

# The possible next-steps for the hill climbing algorithm.
def find_neighbors(ps,lm,tm,opts):
  return swap(ps,lm,tm,opts) + merge(ps,lm,tm,opts) + replace(ps,lm,tm,opts) + split(ps,lm,tm,opts)

# Swap each unique pair of phrases,and return as a list of phrases.
def swap(ps,lm,tm,opts):
  swaps = []
  for i in xrange(len(ps)-1):
    for j in xrange(i,len(ps)):
      swapped = copy.deepcopy(ps)
      temp = swapped[i]
      swapped[i] = swapped[j]
      swapped[j] = temp
      swaps.append(swapped)
  return swaps

# For all phrases in the input list,replaces a single phrase with each of its alternative definitions.
def replace(ps,lm,tm,opts):
  replaces = []
  for n,p in enumerate(ps):
    if p[1] in tm:
      ts = tm[p[1]]
      for t in ts:
        if p[0] != t:
          replaced = copy.deepcopy(ps)
          replaced[n] = (t,p[1])
          replaces.append(replaced)
  return replaces

# Merge consecutive source phrases into a single phrase,if the merged translation exists. Currently does bigram and trigram merges.
def merge(ps,lm,tm,opts):
  merges = []
  for i in xrange(1,len(ps)-1):
    f1 = ps[i][1]
    f2 = ps[i+1][1]
    if f1 and f2 and (f1 + f2) in tm:
      for t in tm[f1+f2]:
        merged = copy.deepcopy(ps)
        merged.remove(ps[i+1])
        merged[i] = (t,f1+f2)
        merges.append(merged)
  if len(ps) >= 3:
    for i in xrange(1,len(ps)-2):
      f1 = ps[i][1]
      f2 = ps[i+1][1]
      f3 = ps[i+2][1]
      if f1 and f2 and f3 and (f1 + f2 + f3) in tm:
        for t in tm[f1+f2+f3]:
          merged = copy.deepcopy(ps)
          merged.remove(ps[i+1])
          merged.remove(ps[i+2])
          merged[i] = (t,f1+f2+f3)
          merges.append(merged)
  return merges

# Splits each multiple-word source phrase into two source phrases,if their translation exist. 
def split(ps,lm,tm,opts):
  splits = []
  for n,i in enumerate(ps):
    french_phrase = ps[n][1]
    if french_phrase != None:
      if len(french_phrase) > 1:
        for j in xrange(1,len(french_phrase)):
          s1 = french_phrase[0:j]
          s2 = french_phrase[j:]
          if s1 in tm and s2 in tm:
            for ts1 in tm[s1]:
              for ts2 in tm[s2]:
                spl = copy.deepcopy(ps)
                spl[n] = (ts1,s1)
                spl.insert(n+1,(ts2,s2))
                splits.append(spl)
  return splits

# Returns the english portion.
def print_phrases(phrases):
	s = ""
	for p in phrases:
	  if p[0] != None:
	    s += p[0].english + " "
	return s
       
# Decode a sentence using a non-monotone stack decoder.
def stack_decode(source,lm,tm,opts):
	hypo = namedtuple("hypo","logprob,lm_state,predecessor,phrase,marked,end_i,fphrase")
	marked = [0 for _ in source] 
	initial_hypothesis = hypo(0.0,lm.begin(),None,None,marked,0,None)
	stacks = [{} for _ in source] + [{}]
	stacks[0][lm.begin()] = initial_hypothesis
	for i,stack in enumerate(stacks[:-1]):
		if len(stack) > opts.s:
			toph = max(stack.itervalues(),key=lambda h: h.logprob)
			top = toph.logprob
			threshold = 1.3
			pruned = sorted(filter(lambda h: h.logprob >= threshold*top,stack.itervalues()),key=lambda h: -h.logprob)[:500]
		else:
			pruned = stack.itervalues()
		for hyp in pruned: # prune
        	    # get the translation options for this hypothesis
			options = get_trans_options(hyp,source,lm,tm,opts)

		# for each translation option
			for (phrase,idxs) in options:
			  start_ind = idxs[0]
			  end_ind = idxs[1]
			  # add the log probability from the translation model
			  logprob = hyp.logprob + phrase.logprob
			  lm_state = hyp.lm_state

	       # evaluate the english phrase using the language model
			  for word in phrase.english.split():
			    (lm_state,word_logprob) = lm.score(lm_state,word)
			    logprob += word_logprob
			    logprob += lm.end(lm_state) if end_ind == len(source)-1 else 0.0
			  marked = copy.deepcopy(hyp.marked)
			  for x in xrange(start_ind,end_ind):
			    marked[x] = 1
			  num_marked = len(filter(lambda x: x == 1,marked))
			  tmark = tuple(marked)
			  new_hypothesis = hypo(logprob,lm_state,hyp,phrase,marked,end_ind,source[start_ind:end_ind])
			  if tmark not in stacks[num_marked] or stacks[num_marked][tmark].logprob < logprob: # second case is recombination
			    stacks[num_marked][tmark] = new_hypothesis
	winner = max(stacks[-1].itervalues(),key=lambda h: h.logprob)
	return hypothesis_phrase(winner)

# get translation options for a hypothesis
def get_trans_options(h,f,lm,tm,opts):
  options = []
  for fi in xrange(len(f)):
    for fj in xrange(fi+1,len(f)+1):
      # check if the range is unmarked
      unmarked = all(lambda x: h.marked[x]==0 for m in range(fi,fj))
      if unmarked:
        if f[fi:fj] in tm:
          phrases = tm[f[fi:fj]]
          for p in phrases:
            options.append((p,(fi,fj)))
  return options


optparser = optparse.OptionParser()
optparser.add_option("-i","--input",dest="input",default="data/input",help="File containing sentences to translate (default=data/input)")
optparser.add_option("-t","--translation-model",dest="tm",default="data/tm",help="File containing translation model (default=data/tm)")
optparser.add_option("-l","--language-model",dest="lm",default="data/lm",help="File containing ARPA-format language model (default=data/lm)")
optparser.add_option("-n","--num_sentences",dest="num_sents",default=sys.maxint,type="int",help="Number of sentences to decode (default=no limit)")
optparser.add_option("-k","--translations-per-phrase",dest="k",default=40,type="int",help="Limit on number of translations to consider per phrase (default=1)")
optparser.add_option("-s","--stack-size",dest="s",default=500,type="int",help="Maximum stack size (default=1)")
optparser.add_option("-v","--verbose",dest="verbose",action="store_true",default=False, help="Verbose mode (default=off)")
opts = optparser.parse_args()[0]

tm = models.TM(opts.tm,opts.k)
lm = models.LM(opts.lm)

french = [tuple(line.strip().split()) for line in open(opts.input).readlines()[:opts.num_sents]]

# tm should translate unknown words as-is with probability 1
for word in set(sum(french,())):
  if (word,) not in tm:
    tm[(word,)] = [models.phrase(word,0.0)]

i = 0
for n,f in enumerate(french):
  i = i + 1
  sentence = decode(f,lm,tm,opts)
  print i, sentence
