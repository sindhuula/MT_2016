'''
This file implements Lagrange optimization without phrase swapping
'''
#!/usr/bin/env python
import optparse
import sys
import models
from collections import namedtuple

#Command line input parameters and defaults
optparser = optparse.OptionParser()
optparser.add_option("-i", "--input", dest="input", default="data/input", help="File containing sentences to translate (default=data/input)")
optparser.add_option("-t", "--translation-model", dest="tm", default="data/tm", help="File containing translation model (default=data/tm)")
optparser.add_option("-l", "--language-model", dest="lm", default="data/lm", help="File containing ARPA-format language model (default=data/lm)")
optparser.add_option("-n", "--num_sentences", dest="num_sents", default=sys.maxint, type="int", help="Number of sentences to decode (default=no limit)")
optparser.add_option("-k", "--translations-per-phrase", dest="k", default=20, type="int", help="Limit on number of translations to consider per phrase (default=1)")
optparser.add_option("-s", "--stack-size", dest="s", default=100, type="int", help="Maximum stack size (default=1)")
optparser.add_option("-v", "--verbose", dest="verbose", action="store_true", default=False,  help="Verbose mode (default=off)")

opts = optparser.parse_args()[0]

tm = models.TM(opts.tm, opts.k)
lm = models.LM(opts.lm)
french = [tuple(line.strip().split()) for line in open(opts.input).readlines()[:opts.num_sents]]

#parameters for lagrange optimization
stepsize = 2.0
eta = -1.5
alpha_decrease_rate = 0.9
epsilon = 0.05
iteration_limit = 10
Constraints_limit = 2

# tm should translate unknown words as-is with probability 1
for word in set(sum(french,())):
  if (word,) not in tm:
    tm[(word,)] = [models.phrase(word, 0.0)]

sys.stderr.write("Decoding %s...\n" % (opts.input,))

# Moving monotone decoding to beam_search function and adding information for new parameters
def beam_search(f, u, Constraints):
  # The following code implements a monotone decoding
  # algorithm (one that doesn't permute the target phrases).
  # Hence all hypotheses in stacks[i] represent translations of 
  # the first i words of the input sentence. You should generalize
  # this so that they can represent translations of *any* i words.
  
  def distortion(end, newstart):
    return eta * abs(end + 1 - newstart)

  hypothesis = namedtuple("hypothesis", "logprob, LR_state, predecessor, phrase, y")
  LR_state = namedtuple("LR_state", "l, m, r, lm_state, bitstring")
  
  initial_y = [0 for i in range(len(f))]
  initial_bitstring = 0
  initial_lrstate = LR_state(0, 0, -1, lm.begin(), initial_bitstring)
  initial_hypothesis = hypothesis(0.0, initial_lrstate, None, None, initial_y)
  stacks = [{} for _ in f] + [{}]
  stacks[0][initial_lrstate] = initial_hypothesis
  
  for i, stack in enumerate(stacks[:-1]):
    for h in sorted(stack.itervalues(),key=lambda h: -h.logprob)[:opts.s]: # prune
      for k in xrange(0, len(f)):
        for j in xrange(k+1, len(f) + 1):
          if i + j - k > len(f):
            break
          if f[k:j] in tm:     
            #Update y vector
            new_y = h.y[:]
            for tmp in range(k, j):
              new_y[tmp] += 1
            #Compute the change of u[i] * y[i]            
            violation_change = sum(u[k: j])      
            if distortion(h.LR_state.r, k) > 6:
              continue
            for phrase in tm[f[k:j]]:
              current_lrstate = h.LR_state
              lm_state = current_lrstate.lm_state            
              logprob = h.logprob + phrase.logprob + violation_change + distortion(h.LR_state.r, k)
              #Prune using constraint set, or update new constraint set
              new_bitstring = current_lrstate.bitstring
              flag = False
              for (index, value) in enumerate(Constraints):
                if k<=value and value<j:
                  if (new_bitstring >> index) & 1 == 1:
                    flag = True
                  else:
                    new_bitstring = new_bitstring | (1 << index)
              if flag:
                continue
              for word in phrase.english.split():
                (lm_state, word_logprob) = lm.score(lm_state, word)
                logprob += word_logprob
              if k == current_lrstate.m+1:
                l = current_lrstate.l
                m = j -1
              else:
                if j == current_lrstate.l:
                  l = k
                  m = current_lrstate.m
                else:
                  l = k
                  m = j -1
              new_n = i + j - k
              r = j - 1
              new_lrstate = LR_state(l, m, r, lm_state, new_bitstring)
              if new_n == len(f):
                logprob += lm.end(lm_state)
              new_hypothesis = hypothesis(logprob, new_lrstate, h, phrase, new_y)
              if new_lrstate not in stacks[new_n] or stacks[new_n][new_lrstate].logprob < logprob: # second case is recombination
                stacks[new_n][new_lrstate] = new_hypothesis
  winner = max(stacks[-1].itervalues(), key=lambda h: h.logprob)

  final_state = 1 << len(Constraints)

  #check bitstring
  def isOK(bitstring):
    return bitstring == final_state -1
  flag = False
  for h in sorted(stacks[-1].itervalues(),key=lambda h: -h.logprob)[:opts.s]: # prune
    if isOK(h.LR_state.bitstring):
      winner = h
      flag = True
      break

  #extract english phrase
  def extract_english(h): 
    return "" if h.predecessor is None else "%s%s " % (extract_english(h.predecessor), h.phrase.english)

  ans = (winner, extract_english(winner))
  return ans

#optimization function for Lagrange optimization
def optimization(f, u, Constraints):
  count_violations = [0 for i in range(len(f))]
  previous = -1e10
  lstar = (-1, -1e10)
  converged = False
  
#checking if current vector is optimal
  def checkoptimal(y):
    for yi in y:
      if yi != 1:
        return False
    return True

  def update_count(y):
    for (i,yi) in enumerate(y):
      if yi != 1:
        count_violations[i] += 1
  now = 0
  step = stepsize
  #check for convergence
  while not converged:
    result = beam_search(f, u, Constraints)
    y = result[0].y
    if checkoptimal(result[0].y):
      return result
    #check convergence
    if lstar[1] < result[0].logprob:
      if (result[0].logprob - lstar[1])/(now - lstar[0]) < epsilon:
        converged = True
      else:
        lstar = (now, result[0].logprob)
    #update u
    for (i, yi) in enumerate(y):
      u[i] -= step * (yi - 1)
    #Update stepsize
    if result[0].logprob > previous:
      step = alpha_decrease_rate * step
    previous = result[0].logprob
    now += 1
    if (now - lstar[0] > iteration_limit):
      break
  iter_lim = iteration_limit
  const_lim = Constraints_limit

  # Find the most violated element in vector
  for iter_count in range(iter_lim):
    result = beam_search(f, u, Constraints)
    y = result[0].y
    #update count
    if checkoptimal(y):
      return result
    update_count(y)

    #Update u again
    for (i, yi) in enumerate(y):
      u[i] -= step * (yi - 1)

    #Update stepsize
    if result[0].logprob >previous:
      step = alpha_decrease_rate * step
    previous = result[0].logprob

  new_Constraints = Constraints[:]

  #Add const_lim constraints
  for cl in range(const_lim):
    maxcount = -1
    maxindex = -1
    for (i, vi) in enumerate(count_violations):
      if (i-1) not in new_Constraints and i not in new_Constraints and (i+1) not in new_Constraints:
        if maxcount < vi:
          maxcount = vi
          maxindex = i
    new_Constraints.append(maxindex)
    new_Constraints.sort()
  return optimization(f, u, new_Constraints)  

    
for fsen in french:
      u = [0 for i in range(len(fsen))]
      result = optimization(fsen, u, [])
      print result[1]
